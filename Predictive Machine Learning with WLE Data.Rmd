---
title: "Predicting Activity Type from the WLE Dataset"
author: "C Hales"
date: "March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(randomForest)
```

## Introduction

The purpose of this exercise is to identify human activity recognition based on sensor data.  
The dataset and details of the weight lifting exercises are provided on the [original research](http://groupware.les.inf.puc-rio.br/har). Six males were asked perform 5 different barbell exercises classed "A" to "E", with sensors applied to the arm, forearm, belt and dumbell.  
A [random forest model](https://en.wikipedia.org/wiki/Random_forest) was used for qualitative activity recognition in the original research. The algorithm constructs a multitude of decision trees on a specified set of features. This approach is followed in this exercise after identifying suitable features of the dataset.

## Data Preparation

The data is provided in the form of two comma delimited files: one for training and the other for testing.

```{r dataprep}
pml_training <- read.csv("H:/Coursera/Machine learning/pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
pml_testing <- read.csv("H:/Coursera/Machine learning/pml-testing.csv", na.strings = c("NA", "", "#DIV/0!"))
pml_training$X <- NULL
pml_testing$X <- NULL
trainingDim <- dim(pml_training) 
testingDim <- dim(pml_testing)
```

The training data has $`r trainingDim[1] `$ data points and the testing data has $`r testingDim[1] `$. Observations for each exercise type are broadly equal based upon the below chart.

```{r exercisefreq, echo=FALSE}
#plot frequency of classe data
ggplot(data.frame(pml_training), aes(x=classe))+ geom_bar(stat = "count") + ggtitle("Frequency of Each Exercise")
```

There are $`r dim(pml_training)[2] `$ columns of data but an initial inspection of the dataframe shows a number sparse columns. This is dealt with by removing columns where less than 10% of the column is populated.

```{r preparedata}
#remove sparse columns
remove_list <- vector()
for (i in colnames(pml_training)) {
  n <- sum(!is.na(pml_training[i]))
  if (n < nrow(pml_training) * 0.1) remove_list <- c(remove_list, i)
}
pml_training <- select(pml_training, -which(names(pml_training) %in% remove_list))
#make y factor
pml_training$classe <- as.factor(pml_training$classe)
#removes descriptive data
pml_training_data <- pml_training[,7:59]
#partition into training and validation
set.seed(32323)
intrain <- createDataPartition(pml_training_data$classe, p = 0.7, list = FALSE)
training <- pml_training_data[intrain, ] #70% split
trainingDim <- dim(training)
validation <- pml_training_data[-intrain, ] #30% split
validationDim <- dim(validation)
```

This reduces the number of potential predictors to $`r dim(pml_training)[2] `$. The training set is sliced 70%/30% for validation: $`r trainingDim[1] `$ data points for training and $`r validationDim[1] `$ for validation.

## Cross-validation and Feature Selection

The rfcv() function is used to cross validate the data. The number of predictors is plotted in order of importance to the error rate in order to determine the features to be selected. The measure used is the [Gini impurity index](http://dni-institute.in/blogs/cart-decision-tree-gini-index-explained/).

```{r crossvalidate}
#cross validation
crossval <- rfcv(training[,1:52],training[,53])
crossval$error.cv

#full random forest model to assess all predictors
rfModel <- randomForest(classe ~ ., data=training, ntree=20, importance=T)
importantPred <- importance(rfModel, sort = TRUE)
importantPred <- importantPred[order(-importantPred[,7]),]
importantPred <- importantPred[1:26,,drop = FALSE]
new_train <- training[,c(rownames(importantPred)[1:26],"classe")]
```

```{r errorchart, echo=FALSE}
#plot of error rate by number of variables used in model
with(crossval, plot(n.var, error.cv, log="x", type="o", lwd=2, col="red",
                    xlab="Number of Variables", ylab="Error Rate"))

#title for plot
title(main="Estimated Error Rate")
```

Based upon the above, the most important 26 predictors were on the basis of the 0.779% error rate as the remaining predictors do not increase the prediction significantly.

## Model Prediction

The training data is subsetted on the 26 predictors for the random forest model. This model is then applied to the validation data for prediction to assess the out-of-sample error.

```{r rfmodel}
importantPred <- importance(rfModel, sort = TRUE)
importantPred <- importantPred[order(-importantPred[,7]),]
importantPred <- importantPred[1:26,,drop = FALSE]
new_train <- training[,c(rownames(importantPred)[1:26],"classe")]

new_rfModel <- randomForest(classe ~ ., data=new_train)
validationPred <- predict(new_rfModel,validation)
confusionMatrix(validationPred,validation$classe)

misClassified = function(real, validationPred) {
    sum(validationPred != real)/length(real)
}
errorRate <- misClassified(validation$classe, validationPred)
```

The error rate on the validation data is $`r errorRate `$%.  
Finally the model is applied to the test data to predict the 20 test cases:

```{r testdataprediction}
predict(new_rfModel,pml_testing)
```

##Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.